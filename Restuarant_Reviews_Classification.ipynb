{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmn67Sf74aoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FkGTTSF4tw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################\n",
        "def relu(mat):\n",
        "    return np.maximum(mat, 0)\n",
        "\n",
        "def sigmoid(mat):\n",
        "    sig = lambda x : 1/(1+np.exp(-x))\n",
        "    return sig(mat)\n",
        "\n",
        "def softmax(mat):\n",
        "    y = np.copy(mat)\n",
        "    y = np.exp(y - np.max(y))\n",
        "    s = np.sum(y, axis=1)\n",
        "    \n",
        "    for i in range(s.shape[0]):\n",
        "        y[i] = y[i]/s[i]\n",
        "    return y \n",
        "\n",
        "def linear(mat):\n",
        "    lin = lambda x : x\n",
        "    return lin(mat)\n",
        "\n",
        "########################\n",
        "def relu_deri(z_):\n",
        "    y = np.copy(z_)\n",
        "    y[y>=0] = 1\n",
        "    y[y<0] = 0\n",
        "    return y\n",
        "        \n",
        "def sigmoid_deri(z_):\n",
        "    sig_deri = lambda x : sigmoid(x)*(1-sigmoid(x)) \n",
        "    return sig_deri(z_) \n",
        "\n",
        "def linear_deri(z_):\n",
        "    lin_deri = lambda x: 1\n",
        "    return lin_deri(z_)\n",
        "\n",
        "def softmax_deri(z_):\n",
        "    soft_deri = lambda x : softmax(x)*(1 - softmax(x))\n",
        "    return soft_deri(z_)\n",
        "\n",
        "##################################\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layer_list, activation_list, x, y):\n",
        "        \n",
        "        self.input                 = x\n",
        "        self.y                     = y\n",
        "        self.output                = np.zeros(layer_list[-1])\n",
        "        \n",
        "        self.layers                = layer_list \n",
        "        self.activations           = activation_list             \n",
        "        \n",
        "        self.weights = []\n",
        "        previous = self.input.shape[1]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            current = layer\n",
        "            self.weights.append(np.random.rand(previous, current))\n",
        "            previous = current\n",
        "        \n",
        "        self.z         = [] #Keeps track of Z = WX + B\n",
        "        self.a         = [] #Keeps track of a = g(Z)\n",
        "    \n",
        "    ##################################\n",
        "    def hidden_update(self, idx, hidden):\n",
        "        z_ = np.dot(hidden ,self.weights[idx])\n",
        "        self.z.append(z_)\n",
        "        \n",
        "        if self.activations[idx] == 'relu':\n",
        "            hidden_ = relu(z_)\n",
        "            \n",
        "        elif self.activations[idx] == 'sigmoid':\n",
        "            hidden_ = sigmoid(z_)\n",
        "\n",
        "        elif self.activations[idx] == 'linear':\n",
        "            hidden_ = linear(z_)\n",
        "            \n",
        "        elif self.activations[idx] == 'softmax':  \n",
        "            hidden_ = softmax(z_)\n",
        "        \n",
        "        return hidden_        \n",
        "        \n",
        "    def feedforward(self):        \n",
        "        hidden = self.input\n",
        "        self.a.append(hidden)\n",
        "        \n",
        "        for layer_idx, units in enumerate(self.layers):\n",
        "            hidden = self.hidden_update(layer_idx, hidden)\n",
        "            self.a.append(hidden)\n",
        "        \n",
        "        self.output = self.a[-1]\n",
        "        \n",
        "    ##################################    \n",
        "    def activate_grad(self, idx):\n",
        "        \n",
        "        if self.activations[idx] == 'relu':\n",
        "            grad_ = relu_deri(self.z[idx])\n",
        "            \n",
        "        elif self.activations[idx] == 'sigmoid':\n",
        "            grad_ = sigmoid_deri(self.z[idx])\n",
        "\n",
        "        elif self.activations[idx] == 'linear':\n",
        "            grad_ = linear_deri(self.z[idx])\n",
        "            \n",
        "        elif self.activations[idx] == 'softmax':  \n",
        "            grad_ = softmax_deri(self.z[idx])\n",
        "            \n",
        "        return grad_\n",
        "        \n",
        "    def backpropagation(self):\n",
        "        grad_weights = []\n",
        "        \n",
        "        if self.layers[-1] == 1:\n",
        "            gradients = [(self.y - self.a[-1].T).T * self.activate_grad(-1)]\n",
        "\n",
        "        else:\n",
        "            l = np.zeros(np.array(self.y).shape)\n",
        "            for i in range(np.array(self.y).shape[0]):\n",
        "                r = np.argmax(self.a[-1][i])\n",
        "                l[i][int(r)] = 1\n",
        "                \n",
        "            gradients = [(self.y - l) * self.activate_grad(-1)]\n",
        "        \n",
        "        for i in range(len(self.layers)-1):    \n",
        "            prev_grad = gradients[0].dot(self.weights[-i-1].T) * self.activate_grad(-i-2)\n",
        "            gradients = [prev_grad] + gradients\n",
        "            \n",
        "        grad_weights = [self.a[i].T.dot(d)/self.input.shape[0] for i,d in enumerate(gradients)]\n",
        "        \n",
        "        return grad_weights\n",
        "    \n",
        " ###################################\n",
        "    def train(self, alpha, epochs):\n",
        "        for e in range(epochs):\n",
        "            i = 0\n",
        "            \n",
        "            self.feedforward()\n",
        "            grad_weights = self.backpropagation()\n",
        "            \n",
        "            for idx in range(len(self.layers)):\n",
        "                self.weights[idx] -= alpha*grad_weights[idx]\n",
        "              \n",
        "    \n",
        "    def predict(self, x_test):\n",
        "        self.input = x_test\n",
        "        self.feedforward()\n",
        "        return self.output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELGf5A1QHwxj",
        "colab_type": "text"
      },
      "source": [
        "# IMDB_Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XERlObx1D7H4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "imdb = keras.datasets.imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBA4kaBiCPAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = []\n",
        "for i in train_data:\n",
        "  train_text.append(decode_review(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiNkwX-HB3d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(min_df=0, lowercase=True)\n",
        "vectorizer.fit(train_text)\n",
        "one_hot = vectorizer.transform(train_text).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzlVWdnSDJAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=100)\n",
        "principalComponents = pca.fit_transform(one_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1oLEj_xGksk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = principalComponents[:1000]\n",
        "train_Y = np.array([train_labels]).T[:1000].T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzhwBoEr43Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Network([100,50,1], ['linear', 'relu' ,'sigmoid'], train_X, train_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXjqc37bER3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train(0.001, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHzK1ycYElFn",
        "colab_type": "code",
        "outputId": "5abc7521-e823-419f-b5e9-198f3cdf7a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.predict(principalComponents[1000:1010]).T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99800548, 0.5       , 1.        , 0.5       , 0.5       ,\n",
              "        1.        , 0.5       , 1.        , 0.5       , 0.5       ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeAxEX89HWmN",
        "colab_type": "code",
        "outputId": "17d4efc1-2f3a-48c3-e706-f795ad4562e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels[1000:1010]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 1, 0, 0, 0, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWf2vdviKXtv",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoM-2JdIU0VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = principalComponents[:20000]\n",
        "train_Y = np.array([train_labels]).T[:20000]\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "enc.fit(train_Y)\n",
        "train_Y = enc.transform(train_Y).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52WZAe0yag3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_X = principalComponents[20000:25000]\n",
        "test_Y = enc.transform(np.array([train_labels]).T[20000:25000]).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3syR21eKZ7J",
        "colab_type": "code",
        "outputId": "f6ea599d-ef52-494f-c7cb-b733800603e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "num_steps = 500\n",
        "batch_size = 100\n",
        "display_step = 100\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 50 # 1st layer number of neurons\n",
        "n_hidden_2 = 25 # 2nd layer number of neurons\n",
        "num_input = 100 # Input features\n",
        "num_classes = 2 # Output Classes\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(\"float\", [None, num_input])\n",
        "Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "# Create model\n",
        "def neural_net(x):\n",
        "    # Hidden fully connected layer \n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    # Hidden fully connected layer \n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "# Construct model\n",
        "logits = neural_net(X)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps+1):\n",
        "        \n",
        "        next_batch = (batch_size*step)%train_X.shape[0]    \n",
        "        \n",
        "        batch_x = train_X[next_batch:batch_size+next_batch]\n",
        "        batch_y = train_Y[next_batch:batch_size+next_batch]\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        \n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            \n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: test_X, Y: test_Y}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 149.0353, Training Accuracy= 0.580\n",
            "Step 100, Minibatch Loss= 8.4119, Training Accuracy= 0.820\n",
            "Step 200, Minibatch Loss= 6.0187, Training Accuracy= 0.790\n",
            "Step 300, Minibatch Loss= 5.3068, Training Accuracy= 0.730\n",
            "Step 400, Minibatch Loss= 4.7957, Training Accuracy= 0.810\n",
            "Step 500, Minibatch Loss= 3.5499, Training Accuracy= 0.810\n",
            "Optimization Finished!\n",
            "('Testing Accuracy:', 0.7278)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hqvtQDlZnwO",
        "colab_type": "text"
      },
      "source": [
        "Task\n",
        "- Report accuracy with layer combination of [100, 75, 75, 2], [100, 16, 16, 2] \n",
        "- Add Another Hidden Layer\n",
        "- Compare loss of different optimisation functions(GD, AdaDelta, RMS Prop)\n",
        "- Use F1 Score instead of accuracy\n",
        "- Apply Dropout (use - https://stackoverflow.com/questions/40879504/how-to-apply-drop-out-in-tensorflow-to-improve-the-accuracy-of-neural-network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlpdLTFkbDAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}